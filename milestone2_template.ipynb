{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MILESTONE 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`File name:` milestone2.py\n",
    "\n",
    "`Authors:`\n",
    "- Víctor González\n",
    "- Alvaro Bautista \n",
    "- Alicia Soria \n",
    "- Kamil Czerniak\n",
    "\n",
    "`Date created:` 05/11/2021\n",
    "\n",
    "`Date last modified:` 12/11/2021\n",
    "\n",
    "`Python Version:` 3.9.2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "\n",
    "**1. INTRODUCTION**\n",
    "   * Context\n",
    "   * Project idea\n",
    "   * Project goals\n",
    "   * Motivation\n",
    "   * Feasibility\n",
    "   \n",
    "   \n",
    "   \n",
    "**2. THE DATA**\n",
    "   * Quotebank\n",
    "   * External data\n",
    "    \n",
    "    \n",
    "    \n",
    "**3. PIPELINE**\n",
    "   * Load data\n",
    "   * Examine our data\n",
    "   * Clean up data\n",
    "   * Modeling\n",
    "   * Interpreting\n",
    "   * Storytelling and communication\n",
    "    \n",
    "    \n",
    "    \n",
    "**4. CONCLUSIONS**\n",
    "   * Summary\n",
    "   * Results\n",
    "   * Problems encountered\n",
    "    \n",
    "    \n",
    "    \n",
    "**5. FUTURE LINES**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brief introduction\n",
    "Welcome to Milestone 2 Python notebook.\n",
    "In this notebook we will answer several research questions surrounding the Breixit event.\n",
    "We will employ the Quotebank dataset mainly, as well as additional databases to enrich the data and obtain more\n",
    "complete conclusions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Project idea\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explain in clear, reasonable, and thorough way the project idea\n",
    "Pinpoint and determine the arguments for and against Breixit in different social groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Project goals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clear project goals\n",
    "-\n",
    "-\n",
    "-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Motivation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What story do we want to tell, why?\n",
    "# Critical awareness of the project (social, cultural, political, economic, education. ... impact)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5. Feasibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Justify feasibility given the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. The data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Quotebank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data source is described best by its makers:\n",
    "\n",
    "> Quotebank is a dataset of 178 million unique, speaker-attributed quotations that were extracted from 196 million English news articles crawled from over 377 thousand web domains between August 2008 and April 2020. The quotations were extracted and attributed using Quobert, a distantly and minimally supervised end-to-end, language-agnostic framework for quotation attribution.\n",
    "\n",
    "(*Vaucher, Timoté, Spitz, Andreas, Catasta, Michele, & West, Robert. (2021). Quotebank: A Corpus of Quotations from a Decade of News (1.0) [Data set]. Zenodo. https://doi.org/10.5281/zenodo.4277311 (accessible on November 10, 2021)*)\n",
    "\n",
    "In our case, we will use data collected between January 2015 and April 2020. We decided to put the start date in 2015, as this was the year of General Election in the United Kingdom, where the Conservative Party (which won the majority in the House of Commons) has put a promise of an in-out referendum in its manifesto:\n",
    "\n",
    "> We will negotiate new rules with the EU, so that people will have to be earning here for a number of years before they can claim benefits, including the tax credits that top up low wages. Instead of something-for\u0002nothing, we will build a system based on the principle of something-for-something. We will then put these changes to the British people in a straight in-out referendum on our membership of the European Union by the end of 2017.\n",
    "\n",
    "(*The Conservative Party Manifesto 2015, http://ucrel.lancs.ac.uk/wmatrix/ukmanifestos2015/localpdf/Conservatives.pdf (accessible on November 10, 2021)*)\n",
    "\n",
    "The data source is based on this paper: *Timoté Vaucher, Andreas Spitz, Michele Catasta, and Robert West\n",
    "\"Quotebank: A Corpus of Quotations from a Decade of News\"\n",
    "Proceedings of the 14th International ACM Conference on Web Search and Data Mining (WSDM), 2021.\n",
    "https://doi.org/10.1145/3437963.3441760*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `quoteID:` Primary key of the quotation (format: \"YYYY-MM-DD-{increasing int:06d}\")\n",
    "- `quotation:` Text of the longest encountered original form of the quotation\n",
    "- `date:` Earliest occurrence date of any version of the quotation\n",
    "- `phase:` Corresponding phase of the data in which the quotation first occurred (A-E)\n",
    "- `probas:` Array representing the probabilities of each speaker having uttered the quotation. The probabilities across different occurrences of the same quotation are summed for each distinct candidate speaker and then normalized\n",
    " - `proba:` Probability for a given speaker\n",
    " - `speaker:` Most frequent surface form for a given speaker in the articles where the quotation occurred\n",
    "- `speaker:` Selected most likely speaker. This matches the the first speaker entry in `probas`\n",
    "- `qids:` Wikidata IDs of all aliases that match the selected speaker\n",
    "- `numOccurrences:` Number of time this quotation occurs in the articles\n",
    "- `urls:` List of links to the original articles containing the quotation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. External data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to get more context behind the speakers, we opted to use Wikidata dataset. This dataset, meant primarily for use in Wikimedia projects like Wikipedia or Wiktionary, contains properties and references describing an item, e.g., a person or a country. Because Quotebank uses Wikidata QIDs to refer to speakers, we can easily link persons to their attributes in Wikidata. \n",
    "\n",
    "Wikidata entries can contain an infinite number of attributes and references, so we have decided to use only a small number of attributes, which we can then use for demographic analysis. These attributes are:\n",
    "- gender\n",
    "- date of birth\n",
    "- nationality\n",
    "- occupation\n",
    "- political party\n",
    "- academic degree\n",
    "- what political offices a person candidated for\n",
    "- religion\n",
    "\n",
    "All of these attributes are capable of containing multiple values (e.g., the entry for Angela Merkel marks her nationality as German and East German). \n",
    "\n",
    "This dataset, in its entirety, may have a size of about 100 GB, which is why we decided to use a subset provided by the course (named *speaker_attributes.parquet*). This subset contains these attributes (and a couple more that we opted not to use) for all speakers featured in Quotebank. In addition, we were provided with labels of all Wikidata entries used in the mentioned subset (*wikidata_labels_descriptions_quotebank.csv.bz2*), in order to dereference non-speaker attributes (like gender) more easily. \n",
    "\n",
    "#### References\n",
    "- Wikidata website: https://www.wikidata.org/wiki/Wikidata:Main_Page\n",
    "- Google Drive directory with preprocessed Wikidata dataset: https://drive.google.com/drive/folders/1VAFHacZFh0oxSxilgNByb1nlNsqznUf0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline we are going to follow\n",
    "- Load data\n",
    "- Examine our data\n",
    "- Clean up data\n",
    "- Exploring and visualizing\n",
    "- Modeling\n",
    "- Interpreting our data\n",
    "- Storytelling and communication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We encourage you to use Google Colab for executing this part of notebook - Google Colab has support for Google Drive, which in turn has support for linking to external folders, allowing us to use larger datasets without losing space capacity on our accounts. In addition, step 3 relies on loading speaker attributes to memory before handling them - this requires ~6 GB of RAM, which could be an issue on computers with 8 GB of RAM or less."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each step will generate its own output files, for each year, to limit the strain on following steps and allow analysis of what was removed in each step by comparing pre-step data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by importing libraries that will be used in the pipeline. We also define years to be considered - this will be used to load files for each year and save the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-3dfa15c31c5a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbz2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mYEARS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"2015\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"2016\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"2017\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"2018\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"2019\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"2020\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/pandas/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdependency\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhard_dependencies\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0m__import__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdependency\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mmissing_dependencies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{dependency}: {e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.8/site-packages/numpy/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mmkl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m     \u001b[0m__mkl_version__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"{MajorVersion}.{MinorVersion}.{UpdateVersion}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmkl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_version\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import re, json, bz2\n",
    "import pandas as pd\n",
    "YEARS = [\"2015\", \"2016\", \"2017\", \"2018\", \"2019\", \"2020\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step removes quotes that are not mentioning Brexit and as such are irrelevant to our analysis. We do this by using a regular expression that matches quotes that either contain the phrase \"Brexit\" or the combination of \"leave\", \"leaving\", \"exit\", \"exiting\" and \"EU\" or \"European Union\". We use a streaming approach used [here](https://colab.research.google.com/drive/1NqLFrAWAzKxr2dAWHI7m6Ml3gWGF72cA) in order to reduce the strain on RAM usage (each datasource file is approx. 2 GB in size).\n",
    "\n",
    "**Input:** Compressed JSON file with quotes from Quotebank from given year, e.g., `quotes-{year}.json.bz2`\n",
    "\n",
    "**Output:** Compressed JSON file with quotes mentioning Brexit from Quotebank from given year, e.g., `quotes-{year}-step1.json.bz2`\n",
    "\n",
    "**NOTE**: due to large number of quotes, this step can take long time - possibly over an hour. You have been warned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/quotes-2015.json.bz2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/cz/drxl2w614qg049wmqrzy06ww0000gn/T/ipykernel_22305/3034077729.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mpath_to_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'data/quotes-{year}-step1.json.bz2'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mbz2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_to_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0ms_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mbz2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_to_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0md_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0minstance\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ms_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ada/lib/python3.8/bz2.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(filename, mode, compresslevel, encoding, errors, newline)\u001b[0m\n\u001b[1;32m    320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m     \u001b[0mbz_mode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"t\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 322\u001b[0;31m     \u001b[0mbinary_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBZ2File\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbz_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompresslevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompresslevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m\"t\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ada/lib/python3.8/bz2.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filename, mode, buffering, compresslevel)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPathLike\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_builtin_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_closefp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode_code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/quotes-2015.json.bz2'"
     ]
    }
   ],
   "source": [
    "# Step 1: only keep quotes that contain phrases: Brexit OR ((leaving OR leave OR exiting OR exit) AND (EU or European Union))\n",
    "\n",
    "regex_text = \"(brexit)|((leave|leaving|exit|exiting).*(\\W+eu\\W+|\\W+european union))\"\n",
    "regex = re.compile(regex_text, re.IGNORECASE)\n",
    "\n",
    "for year in YEARS:\n",
    "    path_to_file = f'data/quotes-{year}.json.bz2' \n",
    "    path_to_out = f'data/quotes-{year}-step1.json.bz2'\n",
    "\n",
    "    with bz2.open(path_to_file, 'rb') as s_file:\n",
    "        with bz2.open(path_to_out, 'wb') as d_file:\n",
    "            for instance in s_file:\n",
    "                instance = json.loads(instance)\n",
    "                quotation = instance['quotation'] + \" \" # extracting quotation, space needed to match 'EU' at the end of a sentence\n",
    "                if(regex.match(quotation) is not None):\n",
    "                    d_file.write((json.dumps(instance)+'\\n').encode('utf-8')) # writing to a new file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second step removes quotes for which Quobert has not been able to attribute a speaker. Since the part of this project is to figure out *who* was for or against leaving the European Union, these quotes would not be useful to our analysis. This step should take significantly less time than the previous one - less than a minute.\n",
    "\n",
    "**Input:** Compressed JSON file with quotes mentioning Brexit from Quotebank from given year, e.g., `quotes-{year}-step1.json.bz2`\n",
    "\n",
    "**Output:** Compressed JSON file with quotes mentioning Brexit and with a known speaker from Quotebank from given year, e.g., `quotes-{year}-step2.json.bz2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: only keep quotes that have an attributed speaker\n",
    "for year in YEARS:\n",
    "    path_to_file = f'data/quotes-{year}-step1.json.bz2' \n",
    "    path_to_out = f'data/quotes-{year}-step2.json.bz2'\n",
    "\n",
    "    with bz2.open(path_to_file, 'rb') as s_file:\n",
    "        with bz2.open(path_to_out, 'wb') as d_file:\n",
    "            for instance in s_file:\n",
    "                instance = json.loads(instance)\n",
    "                speaker = instance['speaker'] # Get the speaker's label\n",
    "                if(speaker != \"None\"):\n",
    "                    d_file.write((json.dumps(instance)+'\\n').encode('utf-8')) # writing to the new file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The third and final step involves matching each quote with attributes of its speaker. As mentioned above, this is done by looking into Wikidata subset datasource and grabbing attributes assigned to most likely speaker's Wikidata QID. Because some fields may be empty (due to missing data), we make sure that this case is handled correctly.\n",
    "\n",
    "**Inputs:** \n",
    "- compressed JSON file with quotes mentioning Brexit and with a known speaker from Quotebank from given year, e.g., `quotes-{year}-step2.json.bz2`\n",
    "- Wikidata subset with data regarding speakers from Quotebank, stored as a `.parquet` file, i.e. `speaker_attributes.parquet`\n",
    "- Wikidata subset with labels and descriptions of all references mentioned in `.parquet` Wikidata subset, i.e. `wikidata_labels_descriptions_quotebank.csv.bz2`\n",
    "\n",
    "**Output:** Compressed JSON file with quotes mentioning Brexit and with a known speaker from Quotebank from given year, alongside attributes for speaker of each quote, e.g., `quotes-{year}-step3.json.bz2`\n",
    "\n",
    "**NOTE**: Wikidata `.parquet` file is stored in memory, which could take ~6 GB of your RAM - please consider using Google Colab, which by default provides 12 GB of RAM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: merge data from Quotebank and Wikidata\n",
    "# NOTE: Wikidata parquet requires ~6 GB of RAM available - please use Colab for this step\n",
    "wd_df = pd.read_parquet('data/speaker_attributes.parquet')\n",
    "wd_df.set_index('id', inplace=True)\n",
    "\n",
    "wd_desc_df = pd.read_csv('data/wikidata_labels_descriptions_quotebank.csv.bz2', compression='bz2', index_col='QID')\n",
    "\n",
    "for year in YEARS:\n",
    "    path_to_file = f'data/quotes-{year}-step2.json.bz2' \n",
    "    path_to_out = f'data/quotes-{year}-step3.json.bz2'\n",
    "    index_of_wd_df = set(wd_df.index)\n",
    "    with bz2.open(path_to_file, 'rb') as s_file:\n",
    "        with bz2.open(path_to_out, 'wb') as d_file:\n",
    "            for instance in s_file:\n",
    "                instance = json.loads(instance)\n",
    "                qid = list(index_of_wd_df.intersection(set(instance['qids']))) # Get speaker's QIDs (there can be multiple)\n",
    "                if(len(qid) == 0):\n",
    "                    # Skip quotes with no speaker mentioned in Wikidata\n",
    "                    d_file.write((json.dumps(instance)+'\\n').encode('utf-8')) \n",
    "                    continue\n",
    "                if (wd_df['gender'][qid][0] is not None):\n",
    "                  instance['gender'] = wd_desc_df['Label'][wd_df['gender'][qid][0]].tolist()\n",
    "                else:\n",
    "                  instance['gender'] = []\n",
    "                if (wd_df['date_of_birth'][qid][0] is not None):\n",
    "                  instance['date_of_birth'] = wd_df['date_of_birth'][qid][0].tolist()\n",
    "                else:\n",
    "                  instance['date_of_birth'] = []\n",
    "                if (wd_df['nationality'][qid][0] is not None):\n",
    "                  instance['nationality'] = wd_desc_df['Label'][wd_df['nationality'][qid][0]].tolist()\n",
    "                else:\n",
    "                  instance['nationality'] = []\n",
    "                if (wd_df['occupation'][qid][0] is not None):\n",
    "                  instance['occupation'] = wd_desc_df['Label'][wd_df['occupation'][qid][0]].tolist()\n",
    "                else:\n",
    "                  instance['occupation'] = []\n",
    "                if (wd_df['party'][qid][0] is not None):\n",
    "                  instance['party'] = wd_desc_df['Label'][wd_df['party'][qid][0]].tolist()\n",
    "                else:\n",
    "                  instance['party'] = []\n",
    "                if (wd_df['academic_degree'][qid][0] is not None):\n",
    "                  instance['academic_degree'] = wd_desc_df['Label'][wd_df['academic_degree'][qid][0]].tolist()\n",
    "                else:\n",
    "                  instance['academic_degree'] = []\n",
    "                if (wd_df['candidacy'][qid][0] is not None):\n",
    "                  instance['candidacy'] = wd_desc_df['Label'][wd_df['candidacy'][qid][0]].tolist()\n",
    "                else:\n",
    "                  instance['candidacy'] = []\n",
    "                if (wd_df['religion'][qid][0] is not None):\n",
    "                  instance['religion'] = wd_desc_df['Label'][wd_df['religion'][qid][0]].tolist()\n",
    "                else:\n",
    "                  instance['religion'] = []\n",
    "                d_file.write((json.dumps(instance)+'\\n').encode('utf-8')) # writing to the new file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Examine our data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding what is in data. Get acquainted with the data.\n",
    "- Formats\n",
    "- Distributions\n",
    "- Missing values\n",
    "- Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Justify feasibility given the data\n",
    "- Format is appropriate for analysis\n",
    "- Enough quotes\n",
    "- Quotes addressing the issue\n",
    "- Different speakers involved\n",
    "- Uniformity of speakers (not all from the same speaker)\n",
    "- Long and short sentences\n",
    "- Not many missing quotes\n",
    "- Not many missing speakers\n",
    "- There shouldnt be many correlations between (i dont know)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Clean up data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generic cleaning\n",
    "- Identify duplicates\n",
    "- Remove empty quotes\n",
    "- Remove empty speakers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP cleaning\n",
    "- Remove capital letters, punctuations, emojis, links\n",
    "- Remove quotes not mentioning Breixit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4. Exploring and visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plan for analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For each of the questions include the following (even though it is not enriched in the end for ex):\n",
    "- Various choices of analyses that we thought about but discarded \n",
    "- Consider ways to enrich, filter, transform data according to needs \n",
    "- Final good choice for analysis. Has to be reasonable and sound\n",
    "- Complete necessary descriptive statistic tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# State and describe all of our questions\n",
    "- Q1: Which percentage of the speakers supported or was against Brexit?\n",
    "- Q2: What arguments did the members of each category use to support their beliefs? \n",
    "- Q3: Who were the main supporters of each of the categories? Analyze them according to age, gender, occupation, ...\n",
    "- Q4: How did the opinion towards Brexit change during the 5 year span? Did the arguments of each group also change?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q1: Which percentage of the speakers supported or was against Brexit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q2: What arguments did the members of each category use to support their beliefs? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q3: Who were the main supporters of each of the categories? Analyze them according to age, gender, occupation, ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q4: How did the opinion towards Brexit change during the 5 year span? Did the arguments of each group also change?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5. Modeling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6. Interpreting our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.7. Storytelling and communication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plan for storytelling and communication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Various choices of communication that we thought about but discarded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final good choice for communication. Has to be reasonable and sound.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Summary of the notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Results obtained\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Problems encountered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Future lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
